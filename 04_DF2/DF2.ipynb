{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DF2_Practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "#spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b131d293-c37a-4753-95e3-85a755db6aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dataframes advanced + Delta Lake\n",
    "\n",
    "* Dataframes\n",
    "    * Custom schemas\n",
    "    * Pivot\n",
    "    * Window functions\n",
    "    * Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5787966d-8ae7-4a2c-a7f2-2199099809f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a605b371-d189-4bc6-9eb6-7bed18b9b2d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import a JSON dataset and have a look at the file\n",
    "\n",
    "import requests\n",
    "r = requests.get(\"https://think.cs.vt.edu/corgis/datasets/json/airlines/airlines.json\")\n",
    "\n",
    "print(r.json()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676417d5-1fa4-4563-b466-fab61135b8ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can also make json into Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rows = (Row(**x) for x in r.json())\n",
    "\n",
    "for row in rows:\n",
    "  print(row)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c22a9e-a4da-4b73-8ab2-128d0d9345f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating a dataframe from this JSON string (Python dictionary) gives us many nested MapType columns\n",
    "\n",
    "airlines_df = spark.createDataFrame(r.json()) # Inferring schema from Python dict was deemed to be deprecated, but is now again accepted starting from Spark 3.1\n",
    "#airlines_df2 = spark.createDataFrame(Row(**x) for x in r.json()) # another way of creating df from dict: unpacking each JSON element into a spark Row element\n",
    "\n",
    "display(airlines_df.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d10699ed-12db-4f47-a18b-57aefe56caa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's test some functions on map columns\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "display(airlines_df\n",
    "        .select(F.explode(\"Airport\") # we can try explode - but this does not make sense for this kind of data\n",
    "                #F.col(\"Airport\").getItem(\"Code\").alias(\"AirportCode\") # we can use getItem to fetch values per key - good for small map columns, not good for large/nested map columns\n",
    "                #,F.col(\"Airport\").getItem(\"Name\").alias(\"AirportName\")\n",
    "                #,F.col(\"Airport\")[\"Code\"] # alternative way, referencing Python dictionary style\n",
    "                #F.col(\"Airport.Code\"),F.col(\"Airport.Name\") # alternative, easier to call but may confuse as it looks like struct. But Airport.* does not work for map\n",
    "                #F.map_keys(\"Airport\") # array of keys\n",
    "                #,F.map_values(\"Airport\") # array of values\n",
    "                ,\"*\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2088142-318a-4ccf-a9d6-9c81106753e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# usually, a struct type of column is more useful and easier to access\n",
    "# in this case, we need to custom define a schema when reading in the DataFrame.\n",
    "# this is also recommended for real-life data pipelines, especially in case of large amount of small data files\n",
    "# potential down-side: missing schema evolution\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# The outer part needs to be a StructType\n",
    "# A StructType needs to consist of StructFields\n",
    "# StructFields have 3 parameters: name, type, nullable\n",
    "\n",
    "# Note: you can remove or add parts of schema\n",
    "# Note2: name has to match the key/column name in the dataset.\n",
    "\n",
    "airport_schema = StructType([\n",
    "  StructField(\"Airport\",StructType([\n",
    "    StructField(\"Code\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "  ]),True), \n",
    "  StructField(\"Statistics\",StructType([\n",
    "    StructField(\"Carriers\", StructType([\n",
    "      StructField(\"Names\", StringType(), True),\n",
    "      StructField(\"Total\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"Minutes Delayed\", StructType([\n",
    "      StructField(\"Late Aircraft\", LongType(), True),\n",
    "      StructField(\"National Aviation System\", LongType(), True),\n",
    "      StructField(\"Weather\", LongType(), True),\n",
    "      StructField(\"Carrier\", LongType(), True),\n",
    "      StructField(\"Security\", LongType(), True),\n",
    "      StructField(\"Total\", LongType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"Flights\", StructType([\n",
    "      StructField(\"Delayed\", LongType(), True),\n",
    "      StructField(\"Diverted\", LongType(), True),\n",
    "      StructField(\"Cancelled\", LongType(), True),\n",
    "      StructField(\"On Time\", LongType(), True),\n",
    "      StructField(\"Total\", LongType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"# of Delays\", StructType([\n",
    "      StructField(\"Late Aircraft\", LongType(), True),\n",
    "      StructField(\"National Aviation System\", LongType(), True),\n",
    "      StructField(\"Weather\", LongType(), True),\n",
    "      StructField(\"Carrier\", LongType(), True),\n",
    "      StructField(\"Security\", LongType(), True)\n",
    "    ]), True)\n",
    "  ]),True),\n",
    "  StructField(\"Time\",StructType([\n",
    "    StructField(\"Label\", StringType(), True),\n",
    "    StructField(\"Month\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Month Name\", StringType(), True)\n",
    "  ]),True)\n",
    "  #StructField(\"MyNullTimestamp\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f3c3f5-013c-4972-93c8-90f4b1c834d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can now provide this schema as input in our dataframe creation\n",
    "\n",
    "airport_schema_df = spark.createDataFrame(r.json(), schema=airport_schema) #spark is not inferring schema. This generally runs much faster for big data\n",
    "airport_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(airport_schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c166420d-84e2-4687-84e7-7e80bf22b509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# another way to create a schema is using StructType's \"add\" method\n",
    "\n",
    "airport_add_schema = (StructType()\n",
    "                      .add(\"Airport\", StructType()\n",
    "                          .add(\"Code\", StringType())\n",
    "                          .add(\"Name\", StringType())\n",
    "                          )\n",
    "                      .add(\"Time\", StructType()\n",
    "                          .add(\"Month\",IntegerType())\n",
    "                          .add(\"Year\",IntegerType()))\n",
    "                     )\n",
    "\n",
    "airport_add_schema_df = spark.createDataFrame(r.json(), schema=airport_add_schema)\n",
    "display(airport_add_schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3de399ef-7573-4125-909b-8d9bc8d4a5d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# third method, raw string\n",
    "airport_string_schema = \"Airport STRUCT<Code: STRING, Name: STRING>, Time STRUCT<Month: INTEGER, Year: INTEGER, Date: INTEGER>\" # date will be null, just an example of adding columns which don't exist\n",
    "\n",
    "airport_string_schema_df = spark.createDataFrame(r.json(), schema=airport_string_schema) \n",
    "display(airport_string_schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e062ad80-2131-4259-91de-fcd6fe4def36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# It is now much easier to navigate and manipulate the fields.\n",
    "\n",
    "display(airport_schema_df\n",
    "        .select(\"Airport.*\"\n",
    "               ,\"Statistics.*\"\n",
    "               ,\"*\"\n",
    "               )\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61fef9e0-3794-47e6-8edf-15c469ea99db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d63649e2-8a56-4082-bf9f-24e716f4eeee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pivot table - summarizing a more extensive table, i.e. plotting data points as columns\n",
    "# let's first load in a dataset\n",
    "\n",
    "airbnb_df = spark.read.parquet(\"amsterdam-listings-2018-12-06.parquet\")\n",
    "display(airbnb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e43851a6-f96a-4d36-9754-1e052a2e7dff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# the dataset has many columns. Let's say we are interested in the average prices per city and neighbourhood.\n",
    "# we are also interested in the size of the place - how many people it accommodates\n",
    "\n",
    "display(airbnb_df.select(\"city\"\n",
    "                       , \"neighbourhood\"\n",
    "                       , \"accommodates\"\n",
    "                       , \"price\")\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30cf918b-6f60-451a-9d91-af362dfa258c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(airbnb_df\n",
    "        .select(\"city\", \"neighbourhood\", \"accommodates\", \"price\")\n",
    "        .groupby(\"city\", \"neighbourhood\") # \"row\" \n",
    "        .pivot(\"accommodates\") # columns\n",
    "        .mean(\"price\") # data / values     # possible options: mean, sum, min, max, count\n",
    "        .na.fill(0) # for filling out null values. 0 for counts/sums\n",
    "        .orderBy(F.desc(\"2\")) # for ordering\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaf9a0dc-15d1-4e2b-8010-6200c806a786",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# another pivot example\n",
    "\n",
    "df_wiki = spark.read.parquet(\"pageviews_by_second.parquet\")\n",
    "\n",
    "display(df_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b9a8b0-01bb-4faf-a259-480d6ef292c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_wiki\n",
    "        .selectExpr(\"cast(timestamp as date) as date\"\n",
    "                   ,\"hour(timestamp) as hour\"\n",
    "                   ,\"site\"\n",
    "                   ,\"requests\")\n",
    "        .groupBy(\"date\",\"hour\") #\"date\"\n",
    "        .pivot(\"site\")\n",
    "        .sum(\"requests\")\n",
    "        .orderBy(\"date\",\"hour\") #\"date\" \n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d241577-126c-4ddb-b0a9-f1b85010ed32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SQL window functions\n",
    "\n",
    "_Note: a \"window\" can be many different things. Here we talk about classical SQL window functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1988f5d-bb39-43be-83e3-0ab05b99505c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in a new dataset and have a look at it\n",
    "healthcare_df = spark.read.parquet(\"health_profile_data.snappy.parquet\")\n",
    "display(healthcare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "226bb17c-c10c-4340-ad51-3700ffeae812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we need to import the Window API and instantiate a Window specification object\n",
    "# we need also pyspark.sql.functions (F) for using aggregations and functions  \n",
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"_id\").orderBy(\"dte\") \n",
    "\n",
    "display(healthcare_df\n",
    "       .withColumn(\"row_num\", F.row_number().over(window_spec)) # similarly can use rank and dense_rank\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442522a1-4b5c-41e1-b4dc-3b1feb95d374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# you can use multiple window specs in parallel\n",
    "  \n",
    "window_spec_by_hr = Window.partitionBy(\"_id\").orderBy(\"resting_heartrate\") #for descending, you can use .orderBy(F.desc(\"resting_heartrate\"))\n",
    "\n",
    "display(healthcare_df\n",
    "       .withColumn(\"row_num\", F.row_number().over(window_spec)) # similarly can use rank and dense_rank\n",
    "       .withColumn(\"rank_num\", F.rank().over(window_spec_by_hr))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad161ca-e70f-4ac5-8bca-b01a2e44496f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use lag / lead for viewing \"back\" or \"ahead\" within a partition's rows\n",
    "\n",
    "display(healthcare_df\n",
    "       .withColumn(\"lag_resting_heartrate\", F.lag(\"resting_heartrate\").over(window_spec)) # use for getting previous/next value in partition. \n",
    "       .withColumn(\"lead_resting_heartrate\", F.lead(\"resting_heartrate\", 5, 0).over(window_spec)) # Can define offset and default value\n",
    "       .withColumn(\"diffToPrev\",F.expr(\"resting_heartrate - lag_resting_heartrate\")) # useful for getting the increase/decrease\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7792582-a539-4c3f-a0b8-2b3ce9c0fff8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rolling windows - useful for moving averages, rolling total, etc\n",
    "\n",
    "window_spec_rolling = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(Window.unboundedPreceding, Window.currentRow) # rolling aggregations - everything up to current row\n",
    "window_spec_rolling_last_week = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-6, Window.currentRow) # rolling aggregations, use negative integer for previous rows\n",
    "window_spec_rolling_plusmin2 = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-2, 2) # rolling aggregations, use positive integer for next rows\n",
    "\n",
    "display(healthcare_df\n",
    "       .withColumn(\"row_num\", F.row_number().over(window_spec)) \n",
    "       .withColumn(\"rolling_avg\", F.avg(\"resting_heartrate\").over(window_spec_rolling_last_week)) # using aggregations with window functions\n",
    "       .withColumn(\"row_num_sum\", F.sum(\"row_num\").over(window_spec_rolling_plusmin2))\n",
    "       .withColumn(\"max_BMI_3\", F.max(\"BMI\").over(window_spec_rolling_plusmin2))\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655b6c16-409e-4112-a9be-bbc50fc5b040",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1a2200-97e5-4950-b271-f4cc9e589c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in a small df for demonstration purposes\n",
    "\n",
    "iso_df = (spark.read\n",
    "          .option(\"header\",\"true\")\n",
    "          .option(\"inferSchema\",\"true\")\n",
    "          .csv(\"ISOCountryLookup.csv\")\n",
    "         )\n",
    "display(iso_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ceebbea-7f30-4259-aa9b-c6cb1dcfc4d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# General issue with data lakes / Hive tables / HDFS storage\n",
    "# Hard to do SQL / Data Warehouse-like updates\n",
    "# No ACID compliance (Hive has now in newer versions, but not inherently compatible with Spark)\n",
    "\n",
    "# Delta Lake = open source\n",
    "# ACID, time-travel, optimized performance, ...\n",
    "\n",
    "iso_df.write.mode(\"overwrite\").saveAsTable(\"iso_t\") # default outside of Databricks without format\n",
    "iso_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_iso_t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20fe7ef3-4fca-455e-a74d-09d84fa8adf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"iso_t\"))\n",
    "#display(spark.table(\"delta_iso_t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed135f4-1bcf-4041-88a8-aa5c278b9307",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# updating using spark sql statements\n",
    "#spark.sql(\"UPDATE iso_t SET independentTerritory = 'Yes' WHERE EnglishShortName = 'Antarctica'\") # fails, update not supported\n",
    "spark.sql(\"UPDATE delta_iso_t SET independentTerritory = 'Yes' WHERE EnglishShortName = 'Antarctica'\") # OK\n",
    "\n",
    "display(spark.table(\"delta_iso_t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28aa1aac-b11f-4572-b366-85b020cf5807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# another way is to use delta API, useful for more advanced usecases and simpler programmability\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"delta_iso_t\") \n",
    "\n",
    "delta_table.update(\n",
    "  condition = \"EnglishShortName = 'Greenland'\",\n",
    "  set = { \"independentTerritory\": \"'Yes'\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a853029-1855-4276-8a87-ff2c61409b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"DESCRIBE HISTORY delta_iso_t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32c348dd-9a10-42ba-add0-dee1b14fde8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# view history of table\n",
    "\n",
    "display(delta_table.history()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19772ea0-d0ed-4de9-a228-449fb6153398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## creating dataframe from previous state\n",
    "\n",
    "#display(spark.sql(\"DESCRIBE EXTENDED delta_iso_t\")) # getting the table path\n",
    "\n",
    "# timestamp_df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-03-22 14:55:00\").load(\"file:/home/jovyan/spark-warehouse/delta_iso_t\")\n",
    "# display(timestamp_df)\n",
    "#version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"file:/home/jovyan/spark-warehouse/delta_iso_t\")\n",
    "#display(version_df)\n",
    "\n",
    "# restoring table to previous state\n",
    "\n",
    "#delta_table.restoreToTimestamp('2024-03-22 14:55:00') # restore to a specific timestamp\n",
    "#delta_table.restoreToVersion(0) # restore table to (oldest) version\n",
    "#display(spark.table(\"delta_iso_t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4d9fd9-1d6b-4884-ac51-afe9c2b8b32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Spark SQL Window functions\n",
    "  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html\n",
    "* Delta Lake: \n",
    "  * https://delta.io/\n",
    "  * https://medium.com/datalex/5-reasons-to-use-delta-lake-format-on-databricks-d9e76cf3e77d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c65d7a-9c3e-4c18-b886-140896a5e76a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75173e12-22f5-4848-b44f-e62e34597d05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Task 1\n",
    "\n",
    "Employees' dataset: \"employees.csv\"\n",
    "\n",
    "Using window functions, find the **employees** who have worked in a specific **department** the *longest* and *shortest* time.\n",
    "\n",
    "Resulting dataframe should have 3 columns: employee_name, department, employment_duration\n",
    "\n",
    "Employment_duration should have 2 possible values: \n",
    "* **longest**\n",
    "  * The employee has worked in the department for the longest time. Based on column _active_record_start_\n",
    "* **shortest**\n",
    "  * The employee has worked in the department for the shortest time. Based on column _active_record_start_\n",
    "\n",
    "Resulting dataframe should have total 6 rows.\n",
    "\n",
    "Example df.take(3):</br>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>employee_name</th>\n",
    "    <th>department</th>\n",
    "    <th>employment_duration</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CISNEROS JR, HERBERT</td>\n",
    "    <td>OFFICE</td>\n",
    "    <td>shortest</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CRAVEN, KEVIN J</td>\n",
    "    <td>OFFICE</td>\n",
    "    <td>longest</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>WRIGHT, RONALD G</td>\n",
    "    <td>PRODUCTION</td>\n",
    "    <td>shortest</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Note: </br>\n",
    "Should be doable with 2 window functions and 2 transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0afee002-3810-4c66-a8f7-cbc3113a32ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 528336678317194,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframes advanced and Delta Lake",
   "notebookOrigID": 3667371382687778,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

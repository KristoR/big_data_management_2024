{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext # for RDDs\n",
    "from pyspark.sql import SparkSession # for DFs\n",
    "\n",
    "sc = SparkContext('local', 'DF_Practice') # for RDDs\n",
    "spark = (SparkSession.builder\n",
    "                    .appName('DF_Practice')\n",
    "                    .getOrCreate()\n",
    "        ) # for DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ca125bd-56dd-474f-8924-5936195b6dca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating dataframes\n",
    "\n",
    "Common ways:\n",
    "1. From an RDD\n",
    "2. From a data source (csv, json, parquet, ... , jdbc)\n",
    "3. From a table\n",
    "4. From a SQL statement\n",
    "5. From a Row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "799fb6a0-c098-4139-ba59-b560c326e969",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# From RDD\n",
    "first_rdd = sc.parallelize([\n",
    "  (1, \"Batman\"),\n",
    "  (2, \"Superman\"),\n",
    "  (3, \"Spiderman\")\n",
    "])\n",
    "\n",
    "first_df = spark.createDataFrame(first_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5906506d-4c42-4479-acf4-5b9115879bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's see how the dataframe and its schema look like\n",
    "\n",
    "\n",
    "#first_df.show(5) # basic display, ASCII\n",
    "\n",
    "#display(first_df) # Databricks specific way of showing results\n",
    "\n",
    "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "#first_df\n",
    "#display(first_df) # but this looks nicer now\n",
    "\n",
    "#display(first_df.limit(50).toPandas()) #Not recommended unless dealing with small data, pandas pulls everything into driver node memory\n",
    "\n",
    "#first_df.schema # schema object\n",
    "#first_df.printSchema() # prints schema of dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4f646e9-e898-46b4-9c09-d7a14cd7b8d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe from a data source / file\n",
    "# \"spark.read\" \n",
    "# use \"option\" for defining additional parameters\n",
    "\n",
    "crimes_df = (spark.read\n",
    "             .option(\"sep\", \"\\t\") # separator\n",
    "             .option(\"header\", True) # file has header row\n",
    "             .option(\"inferSchema\", True) # spark tries to infer data types\n",
    "             .csv(\"Chicago-Crimes-2018.csv\") #path\n",
    "            )\n",
    "\n",
    "display(crimes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba2c8801-80fd-4b2b-84a4-548bea5a1f9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark can also read in nested data (json, parquet)\n",
    "\n",
    "events_df = (spark.read\n",
    "             .option(\"inferSchema\", True)\n",
    "             .json(\"events-500k.json\")\n",
    "            )\n",
    "\n",
    "events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a07331d8-a5ff-45c2-ae2b-83980c5d257b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Everything is a Row...\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# You can create a template for row objects, or you can directly create a row.\n",
    "\n",
    "# Directly creating a row:\n",
    "#my_row = Row(ok=1, arr=[\"hello\", 2], more_data=Row(something=1, others=\"done\"))\n",
    "\n",
    "# Using a template:\n",
    "my_data = Row(\"id\", \"product\", \"cost\")\n",
    "\n",
    "first_row = my_data(1, \"mac\", 1000)\n",
    "second_row = my_data(2, \"windows\", 500)\n",
    "third_row = my_data(3, \"linux\", 700)\n",
    "\n",
    "#rdd = sc.parallelize([first_row, second_row, third_row])\n",
    "df = spark.createDataFrame([first_row, second_row, third_row])\n",
    "\n",
    "#rdd.take(3)\n",
    "#df.take(3)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ecbede93-54da-4033-b2de-bb9d0254ab12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Writing dataframes\n",
    "\n",
    "1. Into tables\n",
    "2. Into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e528e34-0397-4668-9e6e-fda8d9eb49b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(events_df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"event_table\")\n",
    ")\n",
    "# this is saved as a global hive table (in Databricks runtime 7+ --> Delta table)\n",
    "# the table will be accessible from other clusters and it is persisted (not in the free Databricks community version)\n",
    "# you can use df.createOrReplaceTempView(\"view_name\") to create a temporary, session and cluster based sql view\n",
    "# Further info: https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b55c1c5c-15a5-476c-b502-adac5f104bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read whole table into dataframe\n",
    "events_table_df = spark.table(\"event_table\")\n",
    "\n",
    "display(events_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5c3abf7-2b47-4eee-8292-8d2520624e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read sql statement into dataframe\n",
    "events_sql_df = spark.sql(\"SELECT device, event_name, CURRENT_DATE() as cdate FROM event_table LIMIT 50\")\n",
    "\n",
    "display(events_sql_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a860924-6685-4b20-836a-52dc6b6a0d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(first_df.write\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .mode(\"overwrite\") # overwrites data if exists. other options \"append\", \"error\", \"ignore\"\n",
    "  .parquet(\"first.parquet\")\n",
    ")\n",
    "\n",
    "# test after running overwrite, append, ignore modes:\n",
    "#display(spark.read.parquet(\"first.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "849a6a35-38b6-4d85-b7c3-bcb5bf5de9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataframe methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d854b74e-55e6-4e48-a9cf-bb61103db934",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Column transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a845e59f-2606-48e1-8b3e-27ca855af6f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating column objects. \n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#F.col(\"device\")\n",
    "#events_df.device\n",
    "#events_df[\"device\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5364253-162c-4177-a6ff-185ceab86cae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting columns\n",
    "\n",
    "#devices_df = events_df.select(\"user_id\", \"device\")\n",
    "#devices_df = events_df.select(F.col(\"user_id\"), F.col(\"device\"))\n",
    "#devices_df = events_df.select(F.col(\"*\")) # selecting all values - SQL syntax\n",
    "#devices_df = events_df.select(F.col(\"device\"), F.col(\"event_name\"), F.col(\"geo.*\")) # selecting nested fields (struct) with dot notation\n",
    "\n",
    "# for array type, we need to explode the fields (every element will get its own row)\n",
    "\n",
    "#devices_df = events_df.select(F.col(\"*\"), F.explode(F.col(\"items\")).alias(\"items\"))\n",
    "#devices_df = events_df.select(F.col(\"*\"), F.explode_outer(F.col(\"items\")).alias(\"items\"))\n",
    "\n",
    "\n",
    "#display(devices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "808a6bff-3fb1-40aa-a8c6-ac953e7b3999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting columns in SQL-like manner, you can use \"in\", \"case\", etc statements\n",
    "\n",
    "devices_df = events_df.selectExpr(\"user_id\", \"device in ('macOS', 'iOS') as apple_user\")\n",
    "#devices_df = events_df.selectExpr(\"user_id\", \"case when device = 'Windows' then 'Microsoft' else 'Other' end as windows_test\")\n",
    "display(devices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "762420b7-ee7f-4976-8c0b-d4f912cc6e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "\n",
    "anonymous_df = events_df.drop(\"user_id\", \"geo\", \"device\")\n",
    "display(anonymous_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f21cc1dd-955d-4d45-8e01-cd52dd480311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# withColumn can be used for adding new column, or replacing an existing column with same name\n",
    "\n",
    "mobile_df = events_df.withColumn(\"mobile\", F.col(\"device\").isin(\"iOS\", \"Android\"))\n",
    "display(mobile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04e9f6b1-109d-4598-9b9b-79d3ab3f10df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# renaming column names\n",
    "\n",
    "location_df = events_df.withColumnRenamed(\"geo\", \"location\")\n",
    "display(location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b610ee6d-776c-4830-99e7-d56b43ed4f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# replacing column values\n",
    "\n",
    "#from pyspark.sql.functions import when\n",
    "\n",
    "warranty_df = (events_df.select(\n",
    "                F.col(\"*\")\n",
    "                , F.when(F.col(\"event_name\") == \"warranty\",\"issue\")\n",
    "                  .when(F.col(\"event_name\") == \"cart\",\"sale\")\n",
    "                  .otherwise(\"other\").alias(\"event_class\")\n",
    "              ))\n",
    "display(warranty_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53ac7a52-633a-492b-b5b8-88b6bdb92011",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Row transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce661302-ee7a-4102-a94f-3a7012830c02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filtering\n",
    "\n",
    "purchases_df = events_df.filter(\"ecommerce.total_item_quantity > 0\")\n",
    "display(purchases_df)\n",
    "\n",
    "#revenue_df = events_df.filter((F.col(\"ecommerce.purchase_revenue_in_usd\").isNotNull()) & (F.col(\"ecommerce.total_item_quantity\") > 1)) \n",
    "#display(revenue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c2275dd-029c-44d2-8870-b29a0de006ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# distinct values\n",
    "\n",
    "#distinct_events_df = events_df.distinct()\n",
    "#display(distinct_events_df)\n",
    "\n",
    "# dropDuplicates can be used for considering only a subset of columns\n",
    "\n",
    "distinct_event_names_df = events_df.dropDuplicates([\"event_name\"])\n",
    "display(distinct_event_names_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b3cf657-3e35-4fc8-8bfe-7bf048289152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating a new dataframe by using n first rows\n",
    "\n",
    "limit_df = events_df.limit(25)\n",
    "display(limit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "702c0613-9091-4fee-9f67-8b4e936f796c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sorting rows\n",
    "\n",
    "increasing_ts_df = events_df.sort(\"event_timestamp\")\n",
    "display(increasing_ts_df)\n",
    "\n",
    "# use .desc() on a column for descending order. Alternatively, import desc from functions and use it on string-definition of column as desc(\"column_name\")\n",
    "\n",
    "#decreasing_ts_df = events_df.sort(F.col(\"event_timestamp\").desc())\n",
    "#display(decreasing_ts_df)\n",
    "\n",
    "# orderBy = sort\n",
    "\n",
    "#ordered_df = events_df.orderBy([\"device\", \"event_timestamp\"]) #.sort([\"device\", \"event_timestamp\"]) \n",
    "#display(ordered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "12605c88-4af5-44af-b039-86f34d773365",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab2a9fca-d535-4972-a0fc-5ebe76e30c99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating grouped objects\n",
    "\n",
    "events_df.groupBy(\"event_name\")\n",
    "#events_df.groupBy(\"geo.state\", \"geo.city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "512b5a23-cc79-4457-b761-a8c76b065ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# group and count\n",
    "\n",
    "event_counts_df = events_df.groupBy(\"event_name\").count()\n",
    "display(event_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a9bc20c-73d2-44d7-b8e1-f39c1cf77aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# average\n",
    "\n",
    "avg_purchase_per_state_df = events_df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\n",
    "display(avg_purchase_per_state_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e36c5612-b765-471b-b1cd-9775385a216f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sum\n",
    "\n",
    "total_purchase_grouped_df = events_df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\")\n",
    "display(total_purchase_grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb995a72-2e42-4529-8740-e05b3081cbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use agg() for applying different types of aggregations\n",
    "# also allows for other transformations on top of the returned column\n",
    "\n",
    "#from pyspark.sql.functions import sum, avg, approx_count_distinct\n",
    "\n",
    "state_purchases_DF = events_df.groupBy(\"geo.state\").agg(F.sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\n",
    "display(state_purchases_DF)\n",
    "\n",
    "#state_aggregates_df = events_df.groupBy(\"geo.state\").agg(\n",
    "#  F.avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n",
    "#  F.approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n",
    "\n",
    "#display(state_aggregates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66d88db1-6dd0-49f1-991e-b1dddae96dff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Datetime functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a819af6b-f4ca-4d75-96e9-9ac740a23e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# casting to datetime\n",
    "\n",
    "timestamp_df = events_df.withColumn(\"event_timestamp_formatted\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "display(timestamp_df)\n",
    "\n",
    "# alternative: import the specific data type\n",
    "\n",
    "#from pyspark.sql.types import TimestampType\n",
    "\n",
    "#timestamp_df = events_df.withColumn(\"event_timestamp_formatted\", (F.col(\"event_timestamp\") / 1e6).cast(TimestampType()))\n",
    "#display(timestamp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "101d4950-26ab-4f7f-b1e5-09324f0b5213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# formatting dates\n",
    "\n",
    "#from pyspark.sql.functions import date_format\n",
    "\n",
    "format_df = (timestamp_df.withColumn(\"date string\", F.date_format(\"event_timestamp_formatted\", \"MMMM dd, yyyy\"))\n",
    "  .withColumn(\"time string\", F.date_format(\"event_timestamp_formatted\", \"HH:mm:ss.SSSSSS\"))\n",
    ") \n",
    "display(format_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e211a89d-219c-48de-9b19-4b1fe9d1b2d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# extracting datepart\n",
    "\n",
    "#from pyspark.sql.functions import year, month, dayofweek, minute, second\n",
    "\n",
    "datetime_df = (timestamp_df.withColumn(\"year\", F.year(F.col(\"event_timestamp_formatted\")))\n",
    "  .withColumn(\"month\", F.month(F.col(\"event_timestamp_formatted\")))\n",
    "  .withColumn(\"dayofweek\", F.dayofweek(F.col(\"event_timestamp_formatted\")))\n",
    "  .withColumn(\"minute\", F.minute(F.col(\"event_timestamp_formatted\")))\n",
    "  .withColumn(\"second\", F.second(F.col(\"event_timestamp_formatted\")))              \n",
    ")\n",
    "\n",
    "display(datetime_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67c3fd41-bd0e-4463-a837-a2a3f5f7a198",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# converting to date\n",
    "\n",
    "#from pyspark.sql.functions import to_date\n",
    "\n",
    "date_df = timestamp_df.withColumn(\"date\", F.to_date(F.col(\"event_timestamp_formatted\")))\n",
    "display(date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f77c8f2a-e978-40e6-9374-098541298866",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# manipulating dates\n",
    "\n",
    "#from pyspark.sql.functions import date_add\n",
    "\n",
    "plus_df = timestamp_df.withColumn(\"plus_two_days\", F.date_add(F.col(\"event_timestamp_formatted\"), 2))\n",
    "display(plus_df)\n",
    "\n",
    "#plus_df = timestamp_df.selectExpr(\"*\",\"event_timestamp_formatted + interval 2 days\") # spark sql allows for +/- interval type of datetime manipulation\n",
    "#display(plus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28864ee3-2e95-4eaf-b063-9a5319c2399c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Complex types\n",
    "\n",
    "\n",
    "* StructType\n",
    "    * Structure of dataframe, incl. nested fields\n",
    "* ArrayType\n",
    "    * List of elements (same type)\n",
    "* MapType\n",
    "    * Key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73aed2c4-3806-4c6c-8136-652f2c4dd5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's import a new dataset\n",
    "\n",
    "sales_df = spark.read.parquet(\"sales.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1e51b3c-adb2-479a-aa19-568d393e932a",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "details_df = (sales_df.withColumn(\"items\", F.explode(\"items\")) # explode array to rows\n",
    "  .select(\"email\", \"items.item_name\")\n",
    "  .withColumn(\"details\", F.split(F.col(\"item_name\"), \" \")) # split from space: returns an array (list)\n",
    ")\n",
    "display(details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c4576f8-c075-4962-9601-b02b5c8c648c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# array_contains returns True if array contains the value\n",
    "# use element_at for getting specific element\n",
    "# NB - 1-based index\n",
    "\n",
    "\n",
    "mattress_df = (details_df.filter(F.array_contains(F.col(\"details\"), \"Mattress\"))\n",
    "  .withColumn(\"size\", F.element_at(F.col(\"details\"), 2))\n",
    "  .withColumn(\"quality\", F.element_at(F.col(\"details\"), 1))\n",
    ")           \n",
    "display(mattress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67f3eb1d-13b6-44a3-b106-6ad627a3fc69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# similar dataset for pillows\n",
    "\n",
    "pillow_df = (details_df.filter(F.array_contains(F.col(\"details\"), \"Pillow\"))\n",
    "  .withColumn(\"size\", F.element_at(F.col(\"details\"), 1))\n",
    "  .withColumn(\"quality\", F.element_at(F.col(\"details\"), 2))\n",
    ")           \n",
    "display(pillow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4169e36a-2014-4718-b061-6b69165525a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unionByName() resolves columns by name\n",
    "# union() resolves by index\n",
    "\n",
    "union_df = (mattress_df.unionByName(pillow_df)\n",
    "  .drop(\"details\"))\n",
    "display(union_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4099e0c8-50e4-4a9e-8db9-3ae4bbaefb97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# collect_set - aggregate function, returns distinct set of items\n",
    "\n",
    "options_df = (union_df.groupBy(\"email\")\n",
    "  .agg(F.collect_set(\"size\").alias(\"size options\"),\n",
    "       F.collect_set(\"quality\").alias(\"quality options\"))\n",
    ")\n",
    "display(options_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f32dde0-3a82-4b78-807d-a19ec9687721",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa90e6a5-bd98-4670-848a-31d85306cbf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's create a new dataframe\n",
    "# distinct users who made a sale = converted\n",
    "\n",
    "converted_users_df = (sales_df.select(\"email\")\n",
    "  .distinct()\n",
    "  .withColumn(\"converted\", F.lit(True))\n",
    ")\n",
    "display(converted_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2afb9eb-b9af-4b19-803c-720739a5c642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's import whole users dataset for joining\n",
    "\n",
    "users_df = spark.read.parquet(\"users.parquet\")\n",
    "\n",
    "# join all users with converted users\n",
    "\n",
    "conversions_df = (users_df.join(converted_users_df, \"email\", \"outer\") # outer join\n",
    "  .filter(F.col(\"email\").isNotNull()) # only keep users who have e-mail address\n",
    "  .na.fill(False) # fill all null values with False (only looks for matching data type columns, eg ignores integer type columns)\n",
    ")\n",
    "display(conversions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d8bfc4f-ac46-4d42-a692-1e6a388763b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's get the cart history for each user\n",
    "\n",
    "carts_df = (events_df.withColumn(\"items\", F.explode(\"items\"))\n",
    "  .groupBy(\"user_id\").agg(F.collect_set(\"items.item_id\").alias(\"cart\"))\n",
    ")\n",
    "display(carts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b68168e1-203e-43c2-a19b-a441364022b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# left join for cart history with email address\n",
    "\n",
    "email_carts_df = conversions_df.join(carts_df, \"user_id\", \"left\")\n",
    "display(email_carts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb727f2a-1f10-4620-aabf-e12cb91f79ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using join hints and especially broadcast join can make joins a lot faster if one dataframe is small\n",
    "\n",
    "# let's create a small dataframe\n",
    "event_type_df = (events_df.select(\"event_name\")\n",
    "        .distinct()\n",
    "        .withColumn(\"event_type\"\n",
    "                    , F.when((F.col(\"event_name\") == \"register\") | (F.col(\"event_name\") == \"login\"), \"initial\")\n",
    "                    .when((F.col(\"event_name\") == \"checkout\") | (F.col(\"event_name\") == \"cart\") | (F.col(\"event_name\") == \"finalize\"), \"purchase\")\n",
    "                    .otherwise(\"other\")\n",
    "                   ))\n",
    "\n",
    "# let's create a large dataframe\n",
    "events_large_df = events_df.join(events_df.limit(1000), \"device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44aa2459-dd77-4110-b110-706946b2e81f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# view and set the auto broadcast join threshold\n",
    "\n",
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) # default setting is to try broadcasting if one dataframe is smaller than 10MB\n",
    "#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # disable automatic broadcast\n",
    "#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10*1024*1024) # restore default setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ca9c4a2-8b23-4b7b-a22c-9a598de5d7af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# try to join the large and small dataframes\n",
    "\n",
    "spark.catalog.clearCache() # clear cache to ensure fair comparison\n",
    "events_large_df.join(event_type_df, \"event_name\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48167a0e-8031-415e-bfc5-a13c5613fde3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join the dataframes using broadcast hint\n",
    "\n",
    "spark.catalog.clearCache() # clear cache to ensure fair comparison\n",
    "events_large_df.join(event_type_df.hint(\"broadcast\"), \"event_name\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb77652f-d6f8-4f66-8bd1-cd7b65ce9b23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### UDFs\n",
    "\n",
    "User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fcfc04f6-50a1-466b-bb1c-a245f34bace8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a simple Python function\n",
    "\n",
    "def firstLetterFunction(email):\n",
    "  return email[0]\n",
    "\n",
    "firstLetterFunction(\"annagray@kaufman.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd8b1e95-a1fb-498f-b96b-794399dbf9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register the function as a Spark UDF \n",
    "\n",
    "firstLetterUDF = F.udf(firstLetterFunction)\n",
    "\n",
    "display(sales_df.select(firstLetterUDF(F.col(\"email\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1159998-6db5-42a5-966d-66450c3cb870",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Geo distance function\n",
    "# https://gist.github.com/rochacbruno/2883505\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def distance(startLat, startLon, endLat, endLon):    \n",
    "    lat1 = startLat\n",
    "    lon1 = startLon\n",
    "    lat2 = endLat\n",
    "    lon2 = endLon\n",
    "    \n",
    "    \n",
    "    #radius = 6371 # km\n",
    "    radius = 6371000 # m\n",
    "    import math\n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = radius * c / 1000 # distance in kilometres\n",
    "\n",
    "    return d\n",
    "\n",
    "geo_distance = F.udf(distance, DoubleType()) # you can define the return type, default is string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd88d964-47a7-4661-9373-180e5e7f963a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's read in a dataset with coordinates of countries' geographical centres  \n",
    "https://raw.githubusercontent.com/google/dspl/master/samples/google/canonical/countries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c164f04c-a920-4110-b0f3-eaab05982a00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the data in as a dataframe\n",
    "\n",
    "countries_df = (spark.read\n",
    "               .option(\"header\",\"true\")\n",
    "               .option(\"inferSchema\",\"true\")\n",
    "               .csv(\"countries.csv\"))\n",
    "\n",
    "display(countries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dbc0cee4-6e13-49b2-855d-b963387407be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's compare distances of each country's geographical centre, using our UDF\n",
    "\n",
    "distance_df = (countries_df\n",
    " .filter(F.col(\"country\")==\"EE\")\n",
    " .join(countries_df # no join keys = cartesian/cross join\n",
    "       #.filter(F.col(\"country\")==\"FI\")\n",
    "       .toDF(\"join_country\",\"join_latitude\",\"join_longitude\",\"join_name\") # we can use .toDF() to rename all columns of a dataframe\n",
    "       .na.drop() # remove countries with null values\n",
    "      )\n",
    " .withColumn(\"distance_in_km\", geo_distance(\"latitude\",\"longitude\",\"join_latitude\",\"join_longitude\")) # here is the UDF\n",
    " #.filter(F.col(\"distance_in_km\").cast(DoubleType())>0)\n",
    " #.orderBy(\"distance_in_km\")\n",
    ")\n",
    "\n",
    "display(distance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ef2ab17-8de4-40d2-bf5a-2597e19eb30f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Based on the sales dataframe used above, create a new dataframe that has the following fields:\n",
    "* order_id\n",
    "* email\n",
    "* purchase_revenue_in_usd\n",
    "* value_added_tax\n",
    "\n",
    "Value_added_tax should be an UDF, calculated as follows:\n",
    "* if purchase_revenue_in_usd > 1500: tax = revenue*0.2\n",
    "* else: tax = revenue*0.1\n",
    "\n",
    "The dataframe should be filtered to only include rows where value_added_tax is above 110.\n",
    "\n",
    "The dataframe should be returned in a descending order based on the value_added_tax column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdb43a5b-371a-49ab-9345-569b7e4fdcb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76b44f99-23bd-4813-83b2-3a9dd70f84a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 2\n",
    "\n",
    "Join the events dataframe from above and the zips dataset (zips.json).\n",
    "\n",
    "The join should be done based on city and state. Note that the join is case-sensitive, so transform the columns accordingly before the join.\n",
    "\n",
    "Return a dataframe which has the following columns:\n",
    "* user_id\n",
    "* latitude\n",
    "* longitude\n",
    "\n",
    "Latitude is element 2 in the \"loc\" column of the zips dataset</br>\n",
    "Longitude is element 1 in the \"loc\" column of the zips dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58be407f-90bf-4ba6-a2f3-83902b60ad44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4810273f-5442-4ef7-a849-67cbac104649",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further reading\n",
    "https://spark.apache.org/docs/latest/api/sql/index.html</br>\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframe",
   "notebookOrigID": 1061204080530756,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

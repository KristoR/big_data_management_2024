{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DF2_Practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b131d293-c37a-4753-95e3-85a755db6aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Delta Lake continued, Intro to Structured Streaming\n",
    "\n",
    "* Delta Lake\n",
    "    * Data Lake to Data Warehouse: merge into\n",
    "    * Updating table definition\n",
    "    * Partition column\n",
    "* Structured Streaming intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29997c12-5edd-4bf4-9d29-e66e6d1c96ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We will use a movies dataset and try to simulate dimensional modelling behaviour\n",
    "\n",
    "# we don't need to create a separate variable for our dataframes\n",
    "# if we have done testing/debugging, we can be more concise\n",
    "\n",
    "(spark.read\n",
    " .option(\"delimiter\", \"::\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .csv(\"movies/movies.dat\")\n",
    " .toDF(\"MovieID\",\"Title\",\"Genres\") # for naming columns\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_movies\")\n",
    ")\n",
    "\n",
    "(spark.read\n",
    " .option(\"delimiter\", \"::\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .csv(\"movies/ratings.dat\")\n",
    " .toDF(\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings\")\n",
    ")\n",
    "\n",
    "(spark.read\n",
    " .option(\"delimiter\", \"::\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .csv(\"movies/users.dat\")\n",
    " .toDF(\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zip-code\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_users\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377629c8-db8b-4027-b7e9-8910cfcf6d37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's separate data into smaller batches for simulating incremental inserts\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2000-01-01','2000-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2000\")\n",
    ")\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2001-01-01','2001-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2001\")\n",
    ")\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2002-01-01','2002-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2002\")\n",
    ")\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2003-01-01','2003-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2003\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea64d44a-dad2-444f-83ad-65e069472902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we can create dimension and fact tables. We will only use year 2000 data for now\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE DimUser USING DELTA AS\n",
    "SELECT *\n",
    ", CAST('2001-01-01' as date) ValidFrom\n",
    ", CAST('9999-12-31' as date) ValidTo\n",
    "FROM source_users\n",
    "WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE DimMovie USING DELTA  AS\n",
    "SELECT *\n",
    ", CAST('2001-01-01' as date) ValidFrom\n",
    ", CAST('9999-12-31' as date) ValidTo\n",
    "FROM source_movies\n",
    "WHERE movieID IN (SELECT movieID FROM source_ratings_2000)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE FactRating USING DELTA  AS\n",
    "SELECT *\n",
    "FROM source_ratings_2000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f4d2995-613d-40bb-b626-3c18bf79a93b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's make some modifications to the data to see how handling slowly changing dimensions would work\n",
    "\n",
    "spark.sql(\"\"\"UPDATE source_users\n",
    "SET Age = 21\n",
    "WHERE Age = 1\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"UPDATE source_users\n",
    "SET `Zip-code` = 12345\n",
    "WHERE `Zip-Code` = 10023\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"UPDATE source_movies\n",
    "SET Genres = 'Comedy'\n",
    "WHERE MovieID = 18\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c63108-a666-4927-9727-e87935713bce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# the MERGE statement is a powerful way of upserting data:\n",
    "# in the first statement, if the userid exists but something about the user has changed, then we invalidate the old data\n",
    "# and if the userid does not exist, then we insert the new user\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO DimUser tgt\n",
    "USING (\n",
    "  SELECT * FROM source_users\n",
    "  WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n",
    "  OR UserID IN (SELECT UserID FROM source_ratings_2001)\n",
    ") src\n",
    "ON tgt.UserID = src.UserID\n",
    "WHEN MATCHED AND (tgt.Gender != src.Gender OR tgt.Age != src.Age OR tgt.Occupation != src.Occupation OR tgt.`Zip-code` != src.`Zip-code`)\n",
    "  THEN UPDATE SET tgt.ValidTo = '2002-01-01'\n",
    "WHEN NOT MATCHED THEN INSERT (UserID, Gender, Age, Occupation, `Zip-code`, ValidFrom, ValidTo) VALUES ( \n",
    "          src.UserID\n",
    "          , src.Gender\n",
    "          , src.Age\n",
    "          , src.Occupation\n",
    "          , src.`Zip-code`\n",
    "          , CAST('2002-01-01' as date) \n",
    "          , CAST('9999-12-31' as date)\n",
    "          )\n",
    "\"\"\")\n",
    "\n",
    "# in this statement, we add new data for all the rows that we invalidated with the previous statement\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO DimUser tgt\n",
    "USING (\n",
    "  SELECT * FROM source_users\n",
    "  WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n",
    "  OR UserID IN (SELECT UserID FROM source_ratings_2001)\n",
    ") src\n",
    "ON tgt.UserID = src.UserID\n",
    "AND tgt.Gender = src.Gender\n",
    "AND tgt.Age = src.Age\n",
    "AND tgt.Occupation = src.Occupation\n",
    "AND tgt.`Zip-code` = src.`Zip-code`\n",
    "WHEN NOT MATCHED THEN INSERT (UserID, Gender, Age, Occupation, `Zip-code`, ValidFrom, ValidTo) VALUES ( \n",
    "          src.UserID\n",
    "          , src.Gender\n",
    "          , src.Age\n",
    "          , src.Occupation\n",
    "          , src.`Zip-code`\n",
    "          , CAST('2002-01-01' as date) \n",
    "          , CAST('9999-12-31' as date)\n",
    "          )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ea1878-cddb-458d-81d5-a62118127c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using the Delta API\n",
    "# the queries follow the same pattern as in the previous cell\n",
    "\n",
    "dimMovieTable = DeltaTable.forName(spark, \"dimMovie\")\n",
    "sourceMoviesDf = spark.sql(\"\"\"\n",
    "SELECT * FROM source_movies\n",
    "WHERE MovieID IN (SELECT MovieID FROM source_ratings_2000)\n",
    "OR MovieID IN (SELECT MovieID FROM source_ratings_2001)\n",
    "\"\"\")\n",
    "\n",
    "dimMovieTable.alias(\"tgt\").merge(\n",
    "  source = sourceMoviesDf.alias(\"src\"),\n",
    "  condition = \"tgt.MovieID = src.MovieID\"\n",
    ").whenMatchedUpdate(\n",
    "  condition = \"tgt.Title != src.Title OR tgt.Genres != src.Genres\",\n",
    "  set = \n",
    "  {\n",
    "   \"ValidTo\": \"cast('2002-01-01' as date)\"\n",
    "  }\n",
    ").whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"MovieID\": \"src.MovieID\",\n",
    "      \"Title\": \"src.Title\",\n",
    "      \"Genres\": \"src.Genres\",\n",
    "      \"ValidFrom\": \"cast('2002-01-01' as date)\",\n",
    "      \"ValidTo\": \"cast('9999-12-31' as date)\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "\n",
    "dimMovieTable.alias(\"tgt\").merge(\n",
    "  source = sourceMoviesDf.alias(\"src\"),\n",
    "  condition = \"tgt.MovieID = src.MovieID AND tgt.Title = src.Title AND tgt.Genres = src.Genres\"\n",
    ").whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"MovieID\": \"src.MovieID\",\n",
    "      \"Title\": \"src.Title\",\n",
    "      \"Genres\": \"src.Genres\",\n",
    "      \"ValidFrom\": \"cast('2002-01-01' as date)\",\n",
    "      \"ValidTo\": \"cast('9999-12-31' as date)\"\n",
    "    }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema evolution - this cell is for reference on how schema is currently \n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c110656-c67a-4fbc-8762-08cb8b911da1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema evolution\n",
    "# our source data now includes a new column - date\n",
    "# we can use mergeSchema to \n",
    "\n",
    "df = (spark.table(\"source_ratings_2001\")\n",
    "      .select(\"*\", \n",
    "              F.col(\"timestamp\")\n",
    "              .cast(\"timestamp\")\n",
    "              .cast(\"date\")\n",
    "              .alias(\"date\")\n",
    "             )\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"FactRating\")\n",
    "     )\n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(\"FactRating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0cf9819-2e58-4cd6-a950-efd19b0d9e8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema overwriting - mergeSchema is probably not what you want\n",
    "# (here, the column names have changed; however, with mergeSchema, the old columns remain)\n",
    "\n",
    "df = (spark.table(\"source_ratings\")\n",
    "     .select(F.col(\"userID\").alias(\"user\"), \n",
    "              F.col(\"timestamp\")\n",
    "              .cast(\"timestamp\")\n",
    "              .alias(\"utc_timestamp\"), \n",
    "              F.col(\"timestamp\")\n",
    "              .cast(\"timestamp\")\n",
    "              .cast(\"date\")\n",
    "              .alias(\"utc_date\")\n",
    "             )\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"FactRating\")\n",
    "     )\n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we used mode(\"overwrite\"), all of the data was overwritten\n",
    "# but the old schema remained as a rudiment. Thus, all of the old columns are now NULL\n",
    "display(spark.table(\"FactRating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49e23263-c4eb-4fcf-be33-1dc3c384c36b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# instead, if you need to completely overwrite the schema, use overwriteSchema\n",
    "# note: append mode is not allowed\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .select(F.col(\"userID\").alias(\"user\"), \n",
    "          F.col(\"timestamp\")\n",
    "          .cast(\"timestamp\")\n",
    "          .alias(\"utc_timestamp\"), \n",
    "          F.col(\"timestamp\")\n",
    "          .cast(\"timestamp\")\n",
    "          .cast(\"date\")\n",
    "          .alias(\"utc_date\")\n",
    "         )\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")\n",
    "#  .partitionBy(\"utc_date\") # column partition - most commonly on date, needs to be not too unique. Ideally something you use in filters a lot. Only makes sense if it would be ~ 1 GB per partition!\n",
    "  .saveAsTable(\"FactRating\")\n",
    " )\n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72792651-d7e3-45df-9a60-d7086844e07b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"FactRating\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about Delta Lake optimizations:  \n",
    "https://docs.delta.io/latest/optimizations-oss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d095919-1574-47e6-bd89-58298304e99f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured streaming\n",
    "\n",
    "Common input/output:\n",
    "  * Kafka (and other distributed commit logs)\n",
    "  * Files (Parquet, ORC, Avro, JSON, ...)\n",
    "  * (Delta) tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a000241-a9e5-4dbd-9e47-e3b4db029f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's start by looking at reading and writing streams to files\n",
    "# we need a schema when reading streams\n",
    "\n",
    "events_schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "streaming_events_df = (spark.readStream\n",
    "  .schema(events_schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1) # used for example purposes, reads in 1 file per trigger\n",
    "  .parquet(\"events_parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd55451d-fefc-4712-bffd-a92391082893",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# you can check if a dataframe has streaming sources\n",
    "# this means some functions are unavailable (eg count)\n",
    "\n",
    "streaming_events_df.isStreaming\n",
    "\n",
    "\n",
    "# displaying data\n",
    "# in Databricks, we can use display(df) for debugging\n",
    "# running locally, you can debug output to console (note, does not work when using Jupyter)\n",
    "#streaming_events_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609a521d-6546-4d7b-80c3-1ead4eff0aea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's create a new dataframe and write it into a file\n",
    "\n",
    "email_df = (streaming_events_df\n",
    "            .filter(F.col(\"traffic_source\") == \"email\")\n",
    "            .withColumn(\"mobile\", F.col(\"device\").isin([\"iOS\", \"Android\"]))\n",
    "            .select(\"user_id\", \"event_timestamp\", \"mobile\")\n",
    "           )\n",
    "\n",
    "checkpoint_path = \"streaming/email_traffic/checkpoint\" \n",
    "output_path = \"streaming/email_traffic/output\"\n",
    "\n",
    "devices_query = (email_df.writeStream\n",
    "  .outputMode(\"append\") # append = only new rows, complete = all rows written on every update, update = only updated rows (used in aggregations, otherwise same as append)\n",
    "  .format(\"parquet\")\n",
    "  .queryName(\"email_traffic_query\") # optional name\n",
    "  .trigger(processingTime=\"10 second\") # how often data is fetched from source\n",
    "  .option(\"checkpointLocation\", checkpoint_path) # used for fault-tolerance. Note: every query needs to have a unique check point location\n",
    "  .start(output_path) # location where the file will be written\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f07a81-32c1-401c-aec4-ce4051aa94bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# monitor the query\n",
    "\n",
    "#devices_query.id # unique per query, persisted when restarted from checkpoint \n",
    "#devices_query.name\n",
    "#devices_query.status # isDataAvailable = new data available, isTriggerActive = trigger actively firing\n",
    "#devices_query.awaitTermination(5) # used in non-Databricks usecases for keeping the thread (streaming query) alive. \n",
    "#devices_query.stop() # use to shut down the streaming query. Especially in Databricks, otherwise cluster will keep awake indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ffc6e36-a824-4911-a47c-9a8487cd0d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can also write it into a delta table\n",
    "\n",
    "checkpoint_path = \"streaming/email_delta/checkpoint\" \n",
    "output_path = \"spark-warehouse/email_streaming\" # note that we set the output path to wherever the DWH catalog path is (in our case, ./spark-warehouse)\n",
    "\n",
    "(email_df.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .queryName(\"email_delta_query\")\n",
    "  .trigger(processingTime=\"30 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default (outside of Databricks), a new table is not created in the Delta metastore. \n",
    "# use the following query to \"register\" the table:\n",
    "\n",
    "spark.sql(\"CREATE TABLE email_streaming USING DELTA LOCATION 'email_streaming'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this is now an EXTERNAL (unmanaged) table\n",
    "\n",
    "spark.sql(\"DESCRIBE EXTENDED email_streaming\")\n",
    "\n",
    "# the main difference is in how the data is handled:\n",
    "# MANAGED --> DROP TABLE --> drops the \"table\" as well as the data in it\n",
    "# EXTERNAL --> DROP TABLE --> drops the \"table\" (data remains in the path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view snapshots of table \n",
    "\n",
    "display(spark.table(\"email_streaming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also check the count of this table increasing\n",
    "\n",
    "spark.table(\"email_streaming\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it was a delta table, we can look at the history\n",
    "\n",
    "display(spark.sql(\"DESCRIBE HISTORY email_streaming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also create views on top of the data \n",
    "(spark\n",
    " .read\n",
    " .format(\"delta\")\n",
    " .load(\"spark-warehouse/email_streaming\")\n",
    " .createOrReplaceTempView(\"email_streaming_vw\")\n",
    ")\n",
    "\n",
    "display(spark.table(\"email_streaming_vw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5d3804-072b-4846-94b5-f0c99699d65a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use spark.streams.active to loop over all active streams\n",
    "# remember to stop streams if not working on them anymore\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4d9fd9-1d6b-4884-ac51-afe9c2b8b32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Delta Lake MERGE INTO:\n",
    "  * https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html\n",
    "* Structured Streaming: \n",
    "  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html\n",
    "  * https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html\n",
    "  * http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "  * https://docs.databricks.com/spark/latest/structured-streaming/production.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f980944-6bcd-4fdc-8081-f5efed6f66dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tasks\n",
    "\n",
    "##### Task 1\n",
    "Load the `employees.csv` dataset into a delta table `DimEmployee` with mapping the following columns:\n",
    "* employee_id\n",
    "* employee_name\n",
    "* department\n",
    "* region\n",
    "* employee_key\n",
    "* active_record (drop this field)\n",
    "* active_record_start (drop this field)\n",
    "* active_record_end (drop this field)\n",
    "* job_id (constant value `1` in initial load)\n",
    "* valid_from (map this to the same value as active_record_start)\n",
    "* valid_to (map this to the same value as active_record_end, if NULL then map it to 9999-12-31)\n",
    "\n",
    "You have to upsert the data from `employees2024.csv`.\n",
    "* Assume that `employee_id` is the unique value per employee.  \n",
    "* The fields `employee_name`, `department` and `region` should be checked for changes.\n",
    "* Update any data that becomes invalid by setting `valid_to` equal to yesterday's date (NB! do not hardcode yesterday's date)\n",
    "  * Remember that only currently _valid_ data should be updated\n",
    "  * Note: you may need an additional merge statement for which you can use this: https://docs.delta.io/latest/delta-update.html#modify-all-unmatched-rows-using-merge\n",
    "* Insert any new data that should be inserted, with `valid_from` set to current date (NB! do not hardcode current date)\n",
    "* For new data, you can hardcode the `valid_to` field to `9999-12-31`\n",
    "* The updates and inserts should have `job_id = 2`\n",
    "* Display the results from the table by selecting all fields `WHERE job_id = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9cca6f-5579-4372-8a37-3dfe1ff96d2e",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2\n",
    "Create a schema and a streaming dataframe for the JSON files in the `gaming_data` dataset.\n",
    "  \n",
    "  \n",
    "Use the following as basis for creating your schema:  \n",
    " |-- eventName: string (nullable = true)  \n",
    " |-- eventParams: struct (nullable = true)  \n",
    " |    |-- amount: double (nullable = true)  \n",
    " |    |-- app_name: string (nullable = true)  \n",
    " |    |-- app_version: string (nullable = true)  \n",
    " |    |-- client_event_time: string (nullable = true)  \n",
    " |    |-- device_id: string (nullable = true)  \n",
    " |    |-- game_keyword: string (nullable = true)  \n",
    " |    |-- platform: string (nullable = true)  \n",
    " |    |-- scoreAdjustment: long (nullable = true)  \n",
    "  \n",
    "Read in 2 files per trigger.\n",
    "  \n",
    "Create a new modified dataframe:\n",
    "* keep only rows where eventName is \"scoreAdjustment\"\n",
    "* select the *game_keyword*, *platform* and *scoreAdjustment* columns from the eventParams struct.  \n",
    "* set trigger to run every 5 seconds.\n",
    "\n",
    "Write the datastream to a delta table called score_adjustments.  \n",
    "Check to make sure that the table has some data - this should also be visible in the cell results.  \n",
    "Show that the number of rows in the table is increasing.  \n",
    "Then stop the datastream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2086678445992806,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Delta Lake, Structured Streaming",
   "notebookOrigID": 2086678445992776,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

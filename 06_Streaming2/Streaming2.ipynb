{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Streaming2_practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "KAFKA_USER = os.getenv('KAFKA_USER')\n",
    "KAFKA_PW = os.getenv('KAFKA_PW')\n",
    "KAFKA_TOPIC = os.getenv('KAFKA_TOPIC')\n",
    "KAFKA_GROUP_PREFIX = os.getenv('KAFKA_GROUP_PREFIX')\n",
    "KAFKA_JAAS_CONFIG = f\"org.apache.kafka.common.security.scram.ScramLoginModule required username='{KAFKA_USER}' password='{KAFKA_PW}';\"\n",
    "\n",
    "# Generate a random UUID\n",
    "KAFKA_GROUP_ID = f\"{KAFKA_GROUP_PREFIX}{uuid.uuid4()}\"\n",
    "\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark configuration note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with some performance configurations \n",
    "\n",
    "#spark.conf.get(\"spark.sql.shuffle.partitions\") # this is the number of partitions when shuffling data. By default it is 200, which may be overkill for smaller clusters\n",
    "\n",
    "#spark._sc.defaultParallelism # this is the default level of parallelism based on the system settings. E.g., amount of cores, cluster size, etc\n",
    "\n",
    "# For most, and especially smaller datasets, it makes sense to reduce the shuffle partitions. \n",
    "# However, if you get OOM for huge datasets, you may need to increase the partitions. There is no golden rule, you need to test and benchmark\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", spark._sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8307cf20-40d9-4eef-9d0b-af50f5901b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured Streaming\n",
    "  \n",
    "  \n",
    "* Kafka \n",
    "* Aggregations\n",
    "* Time windows\n",
    "* Watermarking\n",
    "* Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2068d2-2855-4d9b-a6e1-2e7d57853e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for getting data from Kafka (or other distributed log systems), we need minimum 2 things:\n",
    "# the server\n",
    "# the topic\n",
    "\n",
    "kafka_server = \"dory.srvs.cloudkafka.com:9094\"         # The location of Kafka bootstrap servers\n",
    "\n",
    "orders_df = (spark.readStream                          # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                     # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server)     # Configure the Kafka server name and port\n",
    "  .option(\"kafka.security.protocol\", \"SASL_SSL\")       # auth\n",
    "  .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-512\")     # auth\n",
    "  .option(\"kafka.sasl.jaas.config\", KAFKA_JAAS_CONFIG) # auth\n",
    "  .option(\"kafka.group.id\", KAFKA_GROUP_ID)            # Group id is used for distributing the load between consumers within the same group\n",
    "  .option(\"subscribe\", KAFKA_TOPIC)                    # Subscribe to the Kafka topic\n",
    "  .option(\"startingOffsets\", \"earliest\")               # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)                 # Rate limit on max offsets per trigger interval\n",
    "  .load()                                              # Load the DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/orders/_checkpoint\" \n",
    "table_name = \"orders_s\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_delta_query = (orders_df.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_delta_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "# if you create the table metastore before any data exists then the stream will result in an error as the table is generated with empty schema\n",
    "def create_table_if_exists(output_path,table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60): # you can replace this with while, currently timeouts after about 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\"))>0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{table_name}'\") # table metastore is created once there is some data (.parquet) in the directory\n",
    "                break\n",
    "        except Exception as e:\n",
    "            #print(e) # if you want to see the exceptions, uncomment this\n",
    "            pass\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the data \n",
    "display(spark.table(table_name).limit(5))\n",
    "\n",
    "#key - the data key. Used in state machines, not useful in this case\n",
    "#value - the data, in binary format. This is our JSON payload. We'll need to cast it to STRING.\n",
    "#topic - the topic we are subscribing to\n",
    "#partition - partition.\n",
    "#offset - the offset value. This is per topic, partition, and consumer group\n",
    "#timestamp - the timestamp (commonly, the processing (or ingestion) timestamp)\n",
    "#timestampType - whether timestamp is create time or log append time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a522a2-5e9e-4f8d-a9f2-f5db0012b652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look at the JSON payload\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "orders_payload_df = (spark.table(table_name)\n",
    "                  .select(F.col(\"value\").cast(StringType()))\n",
    "                  )\n",
    "\n",
    "display(orders_payload_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401c2488-760f-4374-930e-d2326fc4e632",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's create a schema for navigating the JSON payload\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"order_id\", StringType(), True),\n",
    "  StructField(\"user_id\", StringType(), True),\n",
    "  StructField(\"product_ids\", ArrayType(StringType(), True), True),\n",
    "  StructField(\"order_timestamp\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17528c23-8656-4562-9fc7-11b2ba6eb7af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we can use \"from_json\" to parse out the message and provide schema\n",
    "\n",
    "orders_json_df = (orders_payload_df\n",
    "                .select(F.from_json(\"value\", schema).alias(\"json\"))\n",
    "                .select(F.col(\"json.*\"))\n",
    "               )\n",
    "\n",
    "display(orders_json_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's cast the ids into integer and create a new df that has the clean payload data \n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "orders_cleaned_df = (orders_df\n",
    "  .select(F.col(\"value\").cast(StringType()))\n",
    "  .select(F.from_json(\"value\", schema).alias(\"json\"))\n",
    "  .select(F.col(\"json.*\"))\n",
    "  .withColumn(\"order_id\", F.col(\"order_id\").cast(IntegerType()))\n",
    "  .withColumn(\"user_id\", F.col(\"user_id\").cast(IntegerType()))\n",
    "  .withColumn(\"product_ids\", F.transform(F.col(\"product_ids\"), lambda x: x.cast(IntegerType())))\n",
    ")\n",
    "\n",
    "orders_cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's cast the ids into integer and create a new query that outputs the users by most purchased products \n",
    "checkpoint_path = \"streaming/orders_most_products/_checkpoint\" \n",
    "table_name = \"orders_most_products\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"))\n",
    "  .groupBy(\"user_id\")\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475b45b8-9b3d-412e-9da5-0291489a5214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d432333a-1335-4fc4-8b5d-1e79f16ebbf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# commonly, you might not want an aggregation of a stream's whole history.\n",
    "# for this purpose, let's use window from functions - NB this is \"time windows\" not \"SQL-like (row) window function\"\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_tumb/_checkpoint\" \n",
    "table_name = \"orders_most_products_tumb\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_tumb_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\")) # Aggregate by user, every 5 minute block. This is a \"tumbling window\"\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_tumb_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c785c-51ac-4780-be1e-184e01fee543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if we want to keep always the latest time window then we can use sliding windows\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051067d9-2417-4562-bf14-5f55b0347200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Non-Kafka part starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acac1e02-d555-4fdc-aa57-0eb83b64c55d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's try with a different, simpler dataset\n",
    "\n",
    "input_path = \"sensor-data\"\n",
    "\n",
    "json_schema = \"time timestamp, action string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f36054-4522-42b8-97aa-7fabeede5da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a dataframe and apply some transformations and aggregation\n",
    "\n",
    "input_df = (spark\n",
    "  .readStream                                 \n",
    "  .schema(json_schema)                       \n",
    "  .option(\"maxFilesPerTrigger\", 1)            \n",
    "  .json(input_path)                           \n",
    ")\n",
    "\n",
    "counts_df = (input_df\n",
    "  .groupBy(F.col(\"action\"),                     # Aggregate by action\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n",
    "  .count()                                    # Count the actions\n",
    "  .select(F.col(\"window.start\").alias(\"start\"), \n",
    "          F.col(\"count\"),                       \n",
    "          F.col(\"action\"))                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea650a6-ed4e-44a2-8eb7-65d73326a046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/counts/_checkpoint\" \n",
    "table_name = \"counts\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "counts_query = (counts_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"counts_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.col(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e200d92c-a0ce-4b7c-be79-3658e2e56dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d968b6b-01a9-480a-8d63-b3e5c0ca159a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "watermarked_stream = \"watermarked_stream\"\n",
    "\n",
    "watermarked_df = (input_df\n",
    "  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n",
    "  .groupBy(F.col(\"action\"),                       # Aggregate by action...\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n",
    "  .count()                                      # For each aggregate, produce a count\n",
    "  .select(F.col(\"window.start\").alias(\"start\"),   # Elevate field to column\n",
    "          F.col(\"count\"),                         # Include count\n",
    "          F.col(\"action\"))                        # Include action\n",
    ")\n",
    "display(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide_wm/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide_wm\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_wm_query = (orders_cleaned_df\n",
    "  .withWatermark(\"order_timestamp\", \"20 minute\")             # Specify a 20-minute watermark\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_wm_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4aa920-978f-4e07-ab18-efca64d10f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n",
    "\n",
    "schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "hourlyEventsPath = \"events20200703\"\n",
    "\n",
    "website_df = (spark.readStream\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .json(hourlyEventsPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d948ed-3a26-45e7-8e92-836cc07ae9c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n",
    "\n",
    "events_df = (website_df\n",
    "             .withColumn(\"createdAt\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "             .withWatermark(\"createdAt\", \"2 hours\")\n",
    ")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7b62a6-8a6d-41bc-a226-5c5d7cf40943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we can do an aggregation\n",
    "\n",
    "traffic_df = (events_df\n",
    "             .groupBy(\"traffic_source\"\n",
    "                      , F.window(F.col(\"createdAt\"), \"1 hour\"))\n",
    "             .agg(F.approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "             .select(F.col(\"traffic_source\")\n",
    "                     , F.col(\"active_users\")\n",
    "                     , F.hour(F.col(\"window.start\")).alias(\"hour\"))\n",
    "             .sort(\"hour\")\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"streaming/traffic/_checkpoint\" \n",
    "table_name = \"traffic\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "traffic_query = (traffic_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"traffic_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307411ab-59c4-4026-b6d8-65b745ca1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2d54db-d035-4163-a9af-679884a3edb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in users dataset\n",
    "\n",
    "users_df = spark.read.parquet(\"users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600645dc-8cb3-47ba-8d28-bc6330cefdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join works same way as with regular dataframes.\n",
    "# note: this is streaming<->static join\n",
    "\n",
    "joined_df = (events_df\n",
    "            .join(users_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_static/_checkpoint\" \n",
    "table_name = \"join_static\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_static_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_static_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8f8dbd-c601-4ed6-b96a-0af6abbabf36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's read in users dataframe as a stream\n",
    "\n",
    "# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\n",
    "users_schema = users_df.schema\n",
    "\n",
    "users_stream_df = (spark\n",
    "                   .readStream\n",
    "                   .format(\"parquet\")\n",
    "                   .schema(users_schema)\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\n",
    "                   .parquet(\"users.parquet\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab5049a-272a-468b-a80a-0b25830b93fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's do a stream to stream join\n",
    "\n",
    "joined_streams_df = (events_df\n",
    "            .join(users_stream_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_stream/_checkpoint\" \n",
    "table_name = \"join_stream\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_stream_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_stream_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ddcecc-b7c5-427e-bbd7-fea0f4886160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ffb003-abb3-4689-9ad0-8564ac25e1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \n",
    "https://docs.databricks.com/spark/latest/structured-streaming/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b127445-2b2f-49fb-84fe-fc9063f2450f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Create a streaming dataframe from the data in the following path:  \n",
    "`flights200701stream`\n",
    "\n",
    "The schema should contain\n",
    "* DepartureAt (timestamp)\n",
    "* UniqueCarrier (string)\n",
    "\n",
    "Process only 1 file per trigger.  \n",
    "\n",
    "Aggregate the data by count, using non-overlapping 30 minute windows.  \n",
    "Ignore any data that is older than 6 hours.\n",
    "\n",
    "The output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \n",
    "\n",
    "Save to a delta table, firing the trigger every 5 seconds.\n",
    "\n",
    "Display the table, the output should be sorted ascending by startTime.\n",
    "\n",
    "Once the stream has produced some output, call the stream shutdown function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4204d127-eb1f-4c1f-be71-dd4c638935f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Join the Kafka streaming orders dataframe to the `product.csv` dataset.  \n",
    "Note that Spark assumes that any streaming dataframes refer to a directory, not a specific file.\n",
    "\n",
    "Aggregate the data by sum(price) (`total_price`) and a 2-minute tumbling window.  \n",
    "Add a 10 minute watermark, and store the data in a delta table.\n",
    "\n",
    "Create a view on top of the delta table with the following columns:\n",
    "* product_id\n",
    "* product_name\n",
    "* n_minus_2_window_total_price\n",
    "* n_minus_1_window_total_price\n",
    "* current_window_total_price\n",
    "\n",
    "The total_price columns need to be pivoted based on only the 3 most recent windows. \n",
    "The actual \n",
    "Order the dataset by descending `current_total_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Structured Streaming continued",
   "notebookOrigID": 3621743426785043,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
